{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Notebook settings\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from audio import *\n",
    "import random\n",
    "from fastai.callbacks import *\n",
    "from IPython.core.debugger import set_trace\n",
    "np.random.seed(2)\n",
    "p = Config.data_path()/'freesound-audio-tagging-2019';\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions download -c freesound-audio-tagging-2019 -p {p}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = p/'train_noisy'\n",
    "valid_path = p/'train_curated'\n",
    "test_path = p/'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = AudioTransformConfig()\n",
    "config.remove_silence = True\n",
    "config.silence_threshold = 5\n",
    "config.silence_padding = 1000\n",
    "config.sg_cfg.f_max = 44100//2\n",
    "# config.segment_size = 2000\n",
    "# config.max_segments = 2\n",
    "# config.window_size = 2000\n",
    "\n",
    "# For 224x224 nmels = 224, hop 395\n",
    "# For 128x127 hop 694\n",
    "config.sg_cfg.n_mels = 128\n",
    "config.sg_cfg.n_fft = 224*12\n",
    "config.sg_cfg.hop = 694\n",
    "config.to_db_scale = True\n",
    "config.top_db = 76\n",
    "config.max_to_pad = 2000\n",
    "items = ItemList.from_csv(valid_path, '../train_curated.csv').split_none().label_from_df(cols='labels', label_delim=',')\n",
    "carr = []\n",
    "arr = []\n",
    "\n",
    "# Get one from each class\n",
    "for i in range(len(items.x)):\n",
    "    if not str(items.y[i]) in carr:\n",
    "        carr.append(str(items.y[i]))\n",
    "        arr.append(items.x[i])\n",
    "\n",
    "        \n",
    "print(len(arr))\n",
    "# DODGY: TAP (21) Church Bell(37)  Clapping (88)\n",
    "\n",
    "def preview_tfms(idx):\n",
    "    print(\"Class:\", carr[idx])\n",
    "    prevs = preview_transforms(arr[idx], valid_path, config)\n",
    "    for i,y in prevs: i.show(); print(i.data.shape)\n",
    "        \n",
    "preview_tfms(88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _match_cond(data, cond):\n",
    "    return [i for i, x in enumerate(data) if cond(x)]\n",
    "\n",
    "def nonmulti(items):\n",
    "    indexs = _match_cond(items, lambda x : len(x[1].obj)<2)\n",
    "    return data.train[indexs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = AudioList.from_csv(valid_path, '../train_curated.csv', config=config).split_by_rand_pct(.2, seed=2).label_from_df(cols='labels', label_delim=',')\n",
    "data.train = nonmulti(data.train)\n",
    "# data.valid = nonmulti(data.valid)\n",
    "tfms = get_spectro_transforms(roll=True, num_rows=5, num_cols=5, tmasks=2, max_shift_pct=.1)\n",
    "db = data.transform(tfms).databunch(bs=64)\n",
    "print(db)\n",
    "db.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     35,
     76
    ]
   },
   "outputs": [],
   "source": [
    "# from official code https://colab.research.google.com/drive/1AgPdhSp7ttY18O3fEoHOQKlt_3HJDLi8#scrollTo=cRCaCIb9oguU\n",
    "def _one_sample_positive_class_precisions(scores, truth):\n",
    "    \"\"\"Calculate precisions for each true class for a single sample.\n",
    "\n",
    "    Args:\n",
    "      scores: np.array of (num_classes,) giving the individual classifier scores.\n",
    "      truth: np.array of (num_classes,) bools indicating which classes are true.\n",
    "\n",
    "    Returns:\n",
    "      pos_class_indices: np.array of indices of the true classes for this sample.\n",
    "      pos_class_precisions: np.array of precisions corresponding to each of those\n",
    "        classes.\n",
    "    \"\"\"\n",
    "    num_classes = scores.shape[0]\n",
    "    pos_class_indices = np.flatnonzero(truth > 0)\n",
    "    # Only calculate precisions if there are some true classes.\n",
    "    if not len(pos_class_indices):\n",
    "        return pos_class_indices, np.zeros(0)\n",
    "    # Retrieval list of classes for this sample.\n",
    "    retrieved_classes = np.argsort(scores)[::-1]\n",
    "    # class_rankings[top_scoring_class_index] == 0 etc.\n",
    "    class_rankings = np.zeros(num_classes, dtype=np.int)\n",
    "    class_rankings[retrieved_classes] = range(num_classes)\n",
    "    # Which of these is a true label?\n",
    "    retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n",
    "    retrieved_class_true[class_rankings[pos_class_indices]] = True\n",
    "    # Num hits for every truncated retrieval list.\n",
    "    retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n",
    "    # Precision of retrieval list truncated at each hit, in order of pos_labels.\n",
    "    precision_at_hits = (\n",
    "            retrieved_cumulative_hits[class_rankings[pos_class_indices]] /\n",
    "            (1 + class_rankings[pos_class_indices].astype(np.float)))\n",
    "    return pos_class_indices, precision_at_hits\n",
    "\n",
    "\n",
    "def calculate_per_class_lwlrap(truth, scores):\n",
    "    \"\"\"Calculate label-weighted label-ranking average precision.\n",
    "\n",
    "    Arguments:\n",
    "      truth: np.array of (num_samples, num_classes) giving boolean ground-truth\n",
    "        of presence of that class in that sample.\n",
    "      scores: np.array of (num_samples, num_classes) giving the classifier-under-\n",
    "        test's real-valued score for each class for each sample.\n",
    "\n",
    "    Returns:\n",
    "      per_class_lwlrap: np.array of (num_classes,) giving the lwlrap for each\n",
    "        class.\n",
    "      weight_per_class: np.array of (num_classes,) giving the prior of each\n",
    "        class within the truth labels.  Then the overall unbalanced lwlrap is\n",
    "        simply np.sum(per_class_lwlrap * weight_per_class)\n",
    "    \"\"\"\n",
    "    assert truth.shape == scores.shape\n",
    "    num_samples, num_classes = scores.shape\n",
    "    # Space to store a distinct precision value for each class on each sample.\n",
    "    # Only the classes that are true for each sample will be filled in.\n",
    "    precisions_for_samples_by_classes = np.zeros((num_samples, num_classes))\n",
    "    for sample_num in range(num_samples):\n",
    "        pos_class_indices, precision_at_hits = (\n",
    "            _one_sample_positive_class_precisions(scores[sample_num, :],\n",
    "                                                  truth[sample_num, :]))\n",
    "        precisions_for_samples_by_classes[sample_num, pos_class_indices] = (\n",
    "            precision_at_hits)\n",
    "    labels_per_class = np.sum(truth > 0, axis=0)\n",
    "    weight_per_class = labels_per_class / float(np.sum(labels_per_class))\n",
    "    # Form average of each column, i.e. all the precisions assigned to labels in\n",
    "    # a particular class.\n",
    "    per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) /\n",
    "                        np.maximum(1, labels_per_class))\n",
    "    # overall_lwlrap = simple average of all the actual per-class, per-sample precisions\n",
    "    #                = np.sum(precisions_for_samples_by_classes) / np.sum(precisions_for_samples_by_classes > 0)\n",
    "    #           also = weighted mean of per-class lwlraps, weighted by class label prior across samples\n",
    "    #                = np.sum(per_class_lwlrap * weight_per_class)\n",
    "    return per_class_lwlrap, weight_per_class\n",
    "\n",
    "\n",
    "# Wrapper for fast.ai library\n",
    "def lwlrap(scores, truth, **kwargs):\n",
    "    score, weight = calculate_per_class_lwlrap(to_np(truth), to_np(scores))\n",
    "    return torch.Tensor([(score * weight).sum()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = audio_learner(db, models.resnet18, metrics=[lwlrap], pretrained=True, callback_fns=[CSVLogger, ShowGraph])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find(); learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(20, slice(1e-3, 1e-2))\n",
    "learn.save('stage-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.load('stage-1')\n",
    "data = AudioList.from_csv(valid_path, '../train_curated.csv', config=config).split_by_rand_pct(.2, seed=2).label_from_df(cols='labels', label_delim=',')\n",
    "\n",
    "nmulti = nonmulti(data.train)\n",
    "\n",
    "newitems = []\n",
    "for _ in range(3000): \n",
    "    d1 = nmulti[random.randint(0,len(nmulti)-1)]\n",
    "    d2 = nmulti[random.randint(0,len(nmulti)-1)]\n",
    "    mark = config.cache_dir / (\"c_\"+md5(str(d1[0].path)+str(d2[0].path))+'.wav')\n",
    "    d1[0].sig += d2[0].sig\n",
    "    d1[1].obj += d2[1].obj\n",
    "    d1[1].data += d2[1].data\n",
    "    d1[1].raw += d2[1].raw\n",
    "    d1[0].path = mark\n",
    "    torchaudio.save(str(mark), d1[0].sig[None,:], d1[0].sr)\n",
    "    print(d1[1].raw)\n",
    "    data.train.y.items = np.append(data.train.y.items, l, axis=0)\n",
    "    print(data.train.y.items)\n",
    "#     data.train.x.items = np.append(data.train.x.items, mark)\n",
    "\n",
    "tfms = get_spectro_transforms(roll=True, num_rows=10, num_cols=10, tmasks=2, max_shift_pct=.1)\n",
    "db = data.transform(tfms).databunch(bs=64)\n",
    "learn.data = db\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find(); learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learn.fit_one_cycle(10, slice(1e-4, 1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find(); learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ItemList.from_csv(test_path, '../sample_submission.csv').split_none().label_from_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i,l in test.train:\n",
    "    ai = AudioItem.open(test_path/i)\n",
    "    ai.show()\n",
    "    print(audio_predict(learn, ai))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
